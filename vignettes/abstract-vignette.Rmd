---
title: "abstract-vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{abstract-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
#install.package(devtools)
# devtools::install_github("anranjiao/bis620.2023.final")
library(bis620.2023.final)
```

## 1. Background and motivation

The dataset investigated in this project originates from the PRIME (Panitumumab Randomized Trial in Combination with Chemotherapy for Metastatic Colorectal Cancer to Determine Efficacy). This study was conducted to assess the impacts of panitumumab treatment on top of the standard FOLFOX treatment compared to FOLFOX alone for metastatic colorectal cancer. 

Given the high mortality rate associated with cancer, we adopt a pragmatic approach in this study. The motivation behind this work is to ascertain an approximate timeline for patients until their demise. By providing patients and their families with a rough estimate of the remaining days, it empowers them to make informed decisions based on the patientsâ€™ biological information and the chosen course of treatment.

This study employs multiple machine learning models, encompassing linear regression, random forest, and neural networks, to forecast the duration of survival for patients since the start of the treatment. Comprehensive comparisons among each method are offered such as correlation coefficients for each patient information and the ablation study. The study specifically focuses on a cohort of patients who deceased during the study period of the treatment. Multiple features, including age, sex, race, weight, height, genetic information, and the type of treatments, are employed for training these models on a patient-by-patient basis.

## 2. Research question

Given comprehensive details on patients' biological information and the treatment type, can machine learning models accurately forecast the duration of patient survival until their demise? Can these information provide further insights into predicting side effects arising from differnet types of treatments?

## 3. Data cleaning and exploration

Our initial data cleaning process involves merging two tables, namely the "adls" and "biomark" tables, by the subject ID as the identifier. The "adls" table contains personal information such as body weight, height, and the day of death (label we aim to predict). By incorporating the "biomark" table, we augment the dataset with genetic information, which has been observed to significantly influence the efficacy of different treatment types. To focus on estimating the death date, we filter out patients who are still alive.

```{r}
data("adsl")
data("biomark")
dataset = preprocessing(adsl, biomark, 600) # Train from 600 samples out of 687
```

Prior to utilizing the dataset for training, a data preprocessing step involves converting categorical data into numeric values. Majority of machine learning algorithms require both input and output data to be in numeric form. To address this requirement, we employ integer encoding. For instance, the "sex" column containing character values "Male" or "Female" is transformed into numerical equivalents, 1 or 0, respectively. This conversion process extends to any categorical features within the dataset.

Following the merger of pertinent tables and the selection of subject IDs associated with deceased individuals, the dataset is divided into features and labels for model training. We use a roughly 9:1 split ratio is for training and testing. A validation set is omitted due to the limited number of samples available. To expedite convergence, the dataset undergoes standardization, which aligns the scale of each feature. This not only prevents certain features from dominating the learning process but also enhances numerical stability.

```{r}
data_all=dataset$x_train
data_all$DTHDY=dataset$y_train
cor=cor(data_all)
cor_df = as.data.frame(cor[,'DTHDY'])
cor_df['DTHDY'] = NULL #DTHDY always 
abs(cor_df)
```

For a systematic comprehension of the significance of each feature, we calculate a correlation coefficient for each feature utilized in predicting the time of death. As depicted in the figure above, features X, Y, and Z exhibit the highest correlation. This insight prompts further analysis into the predictive capabilities of the models through the execution of an ablation study.

B_ECOG_Fully active: This feature is a categorical value on ECOG Performance Status Scale whether the patient is fully active. This has high correlation with the 'DTHDY', death day, because whether the patient is active or not during the treatment is the strong indicator of the patient's health. If the patient is severely ill, the patient would not be active.

BMMTR2_Mutant: This features is also a categorical value on whether the patient genetic mutation is present on BMMTR2. This has the least correlation with a death day meaning that this mutation has weak correlation with respect to predicting the patient's survival days. 

## 4. Analysis

```{r, fig.width=7, fig.height=4}
# LR
lr_output = linear_model(dataset)
print(lr_output$mae_test)
plot_histogram(lr_output, 12)
```

```{r, fig.width=7, fig.height=4}
# RF
rf_output = random_forest(dataset)
print(rf_output$mae_test)
plot_histogram(rf_output, 12)
```

```{r, fig.width=7, fig.height=4}
# NN
nn_output = neural_network(dataset, 5) # 5 neurons per hidden layer
print(nn_output$mae_test)
plot_histogram(nn_output, 12)
```

Among the three methods, random forest stands out as the top performer in our study. Although histogram has been chosen to visualize the errors above, the user may decide to use a scatter plot, included in the package (plot_scatter_residual()) instead if desired. Assuming the neural network has undergone sufficient training, it theoretically should exhibit superior performance compared to linear regression because of its ability to handle non-linearity in the data through the inclusion of activation functions, allowing it to capture complex relationships. However, based on the result above, the neural network is the worst model. It requires significant hyperparameter tuning to match the satisfactory accuracy compared to that of linear model. While linear regression assumes a linear relationship between input and output which limits its flexibility, random forest, as an ensemble of decision trees, is also capable of handling nonlinear relationships. This ensemble approach enables random forest to capture intricate patterns in the data, contributing to its competitive performance among the three. Through multiple trees, random forest further reduces bias. The choice between linear model and random forests ultimately depends on the specific characteristics of the data and the complexity of the underlying relationships being modeled. In this case, random performed the best especially since the dataset is consisted of several categorical values which are discrete. Therefore, we use this random forest model for further analysis of feature importance through ablation study.

```{r}
data("adae")
dataset_rash = preprocessing_rash(adsl, biomark, adae, 800)
random_forest_rash(dataset_rash)
```

The treatment may cause certain side effects such as rash. There is an additional function in this package in which such side effect can be predicted based on the current biological and clinical information of the patients. We specifically try to predict whether the patient will have a rash or not using a random forest model as shown above. This has an accuracy over 70% on a test set while training accuracy is above 90%. This additional feature shows potential of the machine learning model to make predictions on other labels.

```{r}
remove_active = ablation_study(dataset, 'B_ECOG_Fully active')
print(remove_active)
remove_race = ablation_study(dataset, 'BMMTR2_Mutant')
print(remove_race)
```

In our ablation study, we systematically removed the feature with the highest correlation with a death day and the one with the lowest. By training the model without each feature individually, we assessed the impact on predictive performance. As anticipated, the exclusion of top feature resulted in a significant degradation of the model's performance (Mean Absolute Error on Test Set approximately from 181 to 185), with the magnitude of the observed coefficients serving as a clear indicator of their importance. This underscores the critical role these features play in the model's ability to make accurate predictions regarding the outcome variable. At the same time, removing BMMTR2_Mutant feature still degraded the random forest model's performance by 1, which means this feature still has a decent impact on the model's capacity. 

## 5. Interpretation and conclusions

This research involves the application of three distinct models to forecast the remaining days of patients until their death. Given the broad spectrum of patients' survival days, spanning from 7 to 1310, we acknowledge that achieving a mean absolute error of 181 on the test set is not highly accurate. This outcome aligns with expectations, considering the inherent difficulty in predicting the future lifespan of a patient based solely on biological and clinical information, coupled with the constraint of a relatively small dataset (600 samples for training).

Despite the inherent challenges, this analysis offers valuable insights into features that strongly correlate with a patient's health. Notably, it highlights that the random forest model performs well for datasets of this nature, especially those containing multiple categorical values. Future work may be conducted on exploring the model's capacity on predicting other labels such as side effects other than rash.

In conclusion, while the current accuracy is modest, machine learning models demonstrate potential in providing patients' families with approximate days of survival. As anticipated, enhancing the predictive performance could be achieved with a larger training sample size. The proficiency of the random forest model, particularly in handling datasets with numerous categorical values, underscores its suitability for such applications. 
